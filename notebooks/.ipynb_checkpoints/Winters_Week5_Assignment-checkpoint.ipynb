{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb40fa7b-9416-4f05-8cb9-c77a2a8bff47",
   "metadata": {},
   "source": [
    "- Wiley Winters\n",
    "- Assignment Week 5\n",
    "- September 25, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165166dd",
   "metadata": {},
   "source": [
    "# DS Automation Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195af74",
   "metadata": {},
   "source": [
    "Using our prepared churn data from week 2:\n",
    "- use TPOT to find an ML algorithm that performs best on the data\n",
    "    - Choose a metric you think is best to use for finding the best model; by default, it is accuracy but it could be AUC, precision, recall, etc. The week 3 FTE has some information on these different metrics.\n",
    "    - REMEMBER: TPOT only finds the optimized processing pipeline and model. It doesn't create the model. \n",
    "        - You can use `tpot.export('my_model_name.py')` (assuming you called your TPOT object tpot) and it will save a Python template with an example of the optimized pipeline. \n",
    "        - Use the template code saved from the `export()` function in your program.\n",
    "- create a Python script/file/module using code from the exported template above that\n",
    "    - create a function that takes a pandas dataframe as an input and returns the probability of churn for each row in the dataframe\n",
    "    - your Python file/function should print out the predictions for new data (new_churn_data.csv)\n",
    "    - the true values for the new data are [1, 0, 0, 1, 0] if you're interested\n",
    "- test your Python module and function with the new data, new_churn_data.csv\n",
    "- write a short summary of the process and results at the end of this notebook\n",
    "- upload this Jupyter Notebook and Python file to a Github repository, and turn in a link to the repository in the week 5 assignment dropbox\n",
    "\n",
    "*Optional* challenges:\n",
    "- return the probability of churn for each new prediction, and the percentile where that prediction is in the distribution of probability predictions from the training dataset (e.g. a high probability of churn like 0.78 might be at the 90th percentile)\n",
    "- use other autoML packages, such as TPOT, H2O, MLBox, etc, and compare performance and features with pycaret\n",
    "- create a class in your Python module to hold the functions that you created\n",
    "- accept user input to specify a file using a tool such as Python's `input()` function, the `click` package for command-line arguments, or a GUI\n",
    "- Use the unmodified churn data (new_unmodified_churn_data.csv) in your Python script. This will require adding the same preprocessing steps from week 2 since this data is like the original unmodified dataset from week 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9d9f0-53e0-415f-8248-6f0adc1cf0fb",
   "metadata": {},
   "source": [
    "Import required packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3f1e11-3ab9-4811-8f9b-2dd3cc852638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "from tpot import TPOTClassifier, TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Suppress warnings for the tpot model selection\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89df8f-53b2-428d-8a9c-ebb7290138e2",
   "metadata": {},
   "source": [
    "Since the datafile location can change, I like to put it in its own cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddaf707-87e7-4c92-8eb6-a640a861e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../data/prepped_churn_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b61fa-dbe4-4ebe-865a-d3137f374cc1",
   "metadata": {},
   "source": [
    "Read in datafile and take a quick look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115d6754-64af-4579-ab07-55539cec6234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7590-VHVEG</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575-GNVDE</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668-QPYBK</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7795-CFOCW</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9237-HQITU</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840-RESVB</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>84.80</td>\n",
       "      <td>1990.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234-XADUH</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>103.20</td>\n",
       "      <td>7362.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801-JZAZL</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29.60</td>\n",
       "      <td>346.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8361-LTMKD</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>74.40</td>\n",
       "      <td>306.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3186-AJIEK</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>105.65</td>\n",
       "      <td>6844.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7043 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tenure  PhoneService  Contract  PaymentMethod  MonthlyCharges  \\\n",
       "customerID                                                                  \n",
       "7590-VHVEG       1             0         0              2           29.85   \n",
       "5575-GNVDE      34             1         1              3           56.95   \n",
       "3668-QPYBK       2             1         0              3           53.85   \n",
       "7795-CFOCW      45             0         1              0           42.30   \n",
       "9237-HQITU       2             1         0              2           70.70   \n",
       "...            ...           ...       ...            ...             ...   \n",
       "6840-RESVB      24             1         1              3           84.80   \n",
       "2234-XADUH      72             1         1              1          103.20   \n",
       "4801-JZAZL      11             0         0              2           29.60   \n",
       "8361-LTMKD       4             1         0              3           74.40   \n",
       "3186-AJIEK      66             1         2              0          105.65   \n",
       "\n",
       "            TotalCharges  Churn  \n",
       "customerID                       \n",
       "7590-VHVEG         29.85      0  \n",
       "5575-GNVDE       1889.50      0  \n",
       "3668-QPYBK        108.15      1  \n",
       "7795-CFOCW       1840.75      0  \n",
       "9237-HQITU        151.65      1  \n",
       "...                  ...    ...  \n",
       "6840-RESVB       1990.50      0  \n",
       "2234-XADUH       7362.90      0  \n",
       "4801-JZAZL        346.45      0  \n",
       "8361-LTMKD        306.60      1  \n",
       "3186-AJIEK       6844.50      0  \n",
       "\n",
       "[7043 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file, index_col='customerID')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6201462-e030-4aed-b42d-54258eff3dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7043 entries, 7590-VHVEG to 3186-AJIEK\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   tenure          7043 non-null   int64  \n",
      " 1   PhoneService    7043 non-null   int64  \n",
      " 2   Contract        7043 non-null   int64  \n",
      " 3   PaymentMethod   7043 non-null   int64  \n",
      " 4   MonthlyCharges  7043 non-null   float64\n",
      " 5   TotalCharges    7043 non-null   float64\n",
      " 6   Churn           7043 non-null   int64  \n",
      "dtypes: float64(2), int64(5)\n",
      "memory usage: 440.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d98ac5-ae33-4b44-83ee-7e10e7e078f0",
   "metadata": {},
   "source": [
    "Break out our train and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d93d14a6-288f-42c0-b6ad-58e051a79f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Churn', axis=1)\n",
    "targets = df['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets,\n",
    "                                                    stratify=targets,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55193a70-d529-4b7e-a493-70b9e19bc652",
   "metadata": {},
   "source": [
    "Running *TPOTClassifier* with setting provided by the Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86fef8a-2e17-4109-8700-86dacb110765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.7996936011008858\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8000725681603165\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8000725681603165\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8000725681603165\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8000725681603165\n",
      "\n",
      "Best pipeline: XGBClassifier(input_matrix, learning_rate=0.1, max_depth=2, min_child_weight=2, n_estimators=100, n_jobs=1, subsample=0.45, verbosity=0)\n",
      "0.7921635434412265\n",
      "CPU times: user 22.9 s, sys: 8.42 s, total: 31.3 s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2,\n",
    "                      n_jobs=-1, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415bb69d-41c9-42f4-b1ff-9bd17bce8779",
   "metadata": {},
   "source": [
    "The *TPOTClassifier* selected the `XGBClassifier()` model as the best choice for this dataset and its default parameters.  I will export it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393f2e4d-2cd1-47b0-80d0-3096f3a0280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('../scripts/xgbc_model.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1a203-0a96-463b-a769-722e09021b51",
   "metadata": {},
   "source": [
    "Set up a XGBClassifier() model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf2740c-1697-4164-b5f4-60d25ee6d914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810109806891329\n",
      "0.7910278250993753\n"
     ]
    }
   ],
   "source": [
    "xgbc = XGBClassifier(learning_rate=0.1, max_depth=2, min_child_weight=2,\n",
    "                     n_estimators=100, n_jobs=-1, subsample=0.45, verbosity=0)\n",
    "xgbc.fit(X_train, y_train)\n",
    "print(xgbc.score(X_train, y_train))\n",
    "print(xgbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225fe63-455f-41fc-8325-941e6e0c3fdd",
   "metadata": {},
   "source": [
    "The train and test scores are within range of the internal CV score. Look at its performance for TPR and a classification matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec08cd5-39f0-4c64-8a1a-fd0bac9552ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.8255977496483825\n"
     ]
    }
   ],
   "source": [
    "predictions = xgbc.predict(X_test)\n",
    "tn, tp, fn, tp = confusion_matrix(y_test, predictions).flatten()\n",
    "print('TPR: '+str(tn /(tn +fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ce0b5c-1867-45a3-bf0c-28a348bb416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.86      1294\n",
      "           1       0.65      0.47      0.54       467\n",
      "\n",
      "    accuracy                           0.79      1761\n",
      "   macro avg       0.74      0.69      0.70      1761\n",
      "weighted avg       0.78      0.79      0.78      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c6cf5-39de-4b7c-8a73-d810f2e84d83",
   "metadata": {},
   "source": [
    "Scores are not bad.  For a classification model maximizing the AUC should improve the model's abiity to predict 0 classes as 0 and 1 classes as 1. <a href=\"https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\">Understanding AUC - ROC Curve</a>. This is using the parameters provided in the lecture, but with *scoring* set to **roc_auc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f457a6df-492e-4c08-943c-0f29f80b6f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8402497539950419\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8402497539950419\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8402497539950419\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8402497539950419\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8402497539950419\n",
      "\n",
      "Best pipeline: ExtraTreesClassifier(input_matrix, bootstrap=True, criterion=entropy, max_features=0.8, min_samples_leaf=19, min_samples_split=5, n_estimators=100)\n",
      "0.8438899350982462\n",
      "CPU times: user 20.9 s, sys: 10.5 s, total: 31.4 s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "auc_tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2,\n",
    "                          n_jobs=-1, scoring='roc_auc', random_state=42)\n",
    "auc_tpot.fit(X_train, y_train)\n",
    "print(auc_tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495dca45-afd8-4982-b35d-cd6e8555e6ed",
   "metadata": {},
   "source": [
    "Not much change in the CV score from the first run of TPOT.  It selected the *ExtraTreesClassifier()* as the model to use when scoring is set to **roc_auc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446e726a-b790-4a40-9443-ea3f3009ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_tpot.export('../scripts/etc_tpot_model.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d22531-bd34-4402-a2e6-909fa2481608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8048087845513063\n",
      "0.7995457126632595\n"
     ]
    }
   ],
   "source": [
    "etc = ExtraTreesClassifier(bootstrap=True, criterion='entropy',\n",
    "                           max_features=0.8, min_samples_leaf=19, \n",
    "                           min_samples_split=5, n_estimators=100)\n",
    "etc.fit(X_train, y_train)\n",
    "print(etc.score(X_train, y_train))\n",
    "print(etc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad203d61-d256-4b67-a460-c7f7e3c3151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.8278745644599304\n"
     ]
    }
   ],
   "source": [
    "predictions = etc.predict(X_test)\n",
    "tn, tp, fn, tp = confusion_matrix(y_test, predictions).flatten()\n",
    "print('TPR: '+str(tn /(tn +fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81497ade-8b37-4146-84e1-a8c3a2b664e9",
   "metadata": {},
   "source": [
    "Slight improvement on TPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f957636-19b6-49da-bd38-574e5fa44d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.87      1294\n",
      "           1       0.67      0.47      0.55       467\n",
      "\n",
      "    accuracy                           0.80      1761\n",
      "   macro avg       0.75      0.69      0.71      1761\n",
      "weighted avg       0.79      0.80      0.79      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98294b85-9d50-4a37-9f4f-87e30476371a",
   "metadata": {},
   "source": [
    "Some improvement on the weighted averages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e02c6-ec4e-41d4-84d1-2999d99c22ad",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "# Summary\n",
    "I started this exercise using the **pycaret** libraries and packages.  It took some time, but I was able to get it to work . . . somewhat.  After watching the lecture, I switched to **tpot**.  It appears to be more stable, but does not encapsulate the pipeline build to the same degree as **pycaret**.  Both techniques produced similar results on the new_churn_data set.  The results for both are [1 0 0 0 0] which do not match the stated answer of [1 0 0 1 0].  Not sure where the problem is and I will conduct a more thorough analysis in the future.\n",
    "\n",
    "The quality of the training, test, and new_data is really important.  If I train a model using six features and the new data has seven, the prediction method will not work.  The method I used in the python script is really tailored to this exercise and on the job, I would take more time to build in error checking routines to make sure that the new data matches the training data.\n",
    "\n",
    "Even with autoML processes, it can still take time to evaluate a set of models to find the best fit for the dataset.  With all of the work I've put on the **tpot** notebook, I still feel **pycaret** has more potential for easily selecting a modle and building a pipeline around it.  Both packages support GPU; therefore, I may enable that feature and see how much performance improvement my kind of old GPU can provide.\n",
    "\n",
    "I noticed that `sklearn.prediction()` returns a *numpy.ndarray* which I wasn't sure how to handle.  I tried something like\n",
    "\n",
    "```\n",
    "for i in range(len(new_data)):\n",
    "   print('Churn = '+(new_data[i], predictions[i])\n",
    "```\n",
    "but recieved a key error, so I just printed the array."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5a5a5dcfe8c405964b888b7eb63d71c041385b923cf8ef6565c4fe595d89b61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
